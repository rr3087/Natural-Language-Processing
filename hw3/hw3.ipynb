{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Name: Rahul Rana\n",
    "\n",
    "Uni: rr3087\n",
    "\n",
    "Email: rr3087@columbia.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import h5py\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: \n",
    "SEED = 3087\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "I'm a Don Johnson fan, but this is undoubtedly the WORST movie, done by anybody, that I've ever seen. The acting was bad, as was the cinematography. D\n",
      "0\n",
      "This is a fan-made short film that pretends to be a preview for a new movie that pairs Batman and Superman! It's the sort of film that fans adore and \n",
      "1\n",
      "***SPOILERS*** ***SPOILERS*** Juggernaut is a British made \"thriller\" released in the US by First National. Karloff is Dr. Sartorius who has to leave \n",
      "0\n",
      "I could write a big enough comment on any one of the characters in Gundam Wing, they could each lead the series with their internal conflicts. Instead\n",
      "1\n",
      "Cabin Fever is the first feature film directed by Eli Roth.Roth and Randy Pearlstein co wrote the script from a story by Roth.this a zombie film,which\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[ 122  188   24  496   36  156 1481 2672    3  171  483  159    1  851    2\n",
      "  688   41    1   80   45]\n",
      "[ 1.  0.]\n",
      "\n",
      "[ 7833     4  5196     2 17949    12    94    22   101    42   125   199\n",
      "  2791     7     7     1  1497   201     1  1072]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=128, input_length=250))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 12s - loss: 0.4058 - acc: 0.7924 - val_loss: 0.2936 - val_acc: 0.8778\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 9s - loss: 0.0524 - acc: 0.9819 - val_loss: 0.4368 - val_acc: 0.8534\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 9s - loss: 0.0026 - acc: 0.9996 - val_loss: 0.5207 - val_acc: 0.8676\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 9s - loss: 1.8796e-04 - acc: 1.0000 - val_loss: 0.5586 - val_acc: 0.8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec974cfd68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=128, input_length=250))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 10s - loss: 0.3969 - acc: 0.8045 - val_loss: 0.2928 - val_acc: 0.8768\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 10s - loss: 0.0413 - acc: 0.9869 - val_loss: 0.5369 - val_acc: 0.8390\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 10s - loss: 0.0054 - acc: 0.9982 - val_loss: 0.7515 - val_acc: 0.8438\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 10s - loss: 0.0163 - acc: 0.9946 - val_loss: 0.7493 - val_acc: 0.8464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec9aef5748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=128, input_length=250))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 235s - loss: 0.4340 - acc: 0.7968 - val_loss: 0.3921 - val_acc: 0.8278\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 238s - loss: 0.2090 - acc: 0.9212 - val_loss: 0.3453 - val_acc: 0.8570\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 235s - loss: 0.1212 - acc: 0.9578 - val_loss: 0.3883 - val_acc: 0.8704\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 233s - loss: 0.0833 - acc: 0.9709 - val_loss: 0.4826 - val_acc: 0.8596\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 238s - loss: 0.0522 - acc: 0.9824 - val_loss: 0.5604 - val_acc: 0.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec3da59a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 35772\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 37,512,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=300, input_length=250, weights=[g_word_embedding_matrix]))\n",
    "model.add(LSTM(128, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 282s - loss: 0.4151 - acc: 0.8039 - val_loss: 0.2638 - val_acc: 0.8974\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 276s - loss: 0.1949 - acc: 0.9261 - val_loss: 0.2611 - val_acc: 0.8974\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 272s - loss: 0.0879 - acc: 0.9700 - val_loss: 0.3466 - val_acc: 0.8878\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 273s - loss: 0.0337 - acc: 0.9896 - val_loss: 0.4659 - val_acc: 0.8842\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 272s - loss: 0.0163 - acc: 0.9947 - val_loss: 0.5414 - val_acc: 0.8826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb94232710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 39,421,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=300, input_length=250, weights=[g_word_embedding_matrix]))\n",
    "model.add(Convolution1D(filters=128, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=64, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=32, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 48s - loss: 0.4722 - acc: 0.7434 - val_loss: 0.2872 - val_acc: 0.8802\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 37s - loss: 0.2352 - acc: 0.9089 - val_loss: 0.2392 - val_acc: 0.9052\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 37s - loss: 0.1423 - acc: 0.9470 - val_loss: 0.2687 - val_acc: 0.8988\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 37s - loss: 0.0817 - acc: 0.9714 - val_loss: 0.3527 - val_acc: 0.8844\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 37s - loss: 0.0455 - acc: 0.9841 - val_loss: 0.3968 - val_acc: 0.8930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1edd5d9d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 39,421,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "### 1st Model\n",
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=300, input_length=250, weights=[g_word_embedding_matrix]))\n",
    "model.add(Convolution1D(filters=128, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=64, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=32, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adadelta',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 55s - loss: 0.5404 - acc: 0.7029 - val_loss: 0.3349 - val_acc: 0.8620\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 47s - loss: 0.3269 - acc: 0.8642 - val_loss: 0.3018 - val_acc: 0.8830\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 45s - loss: 0.2825 - acc: 0.8848 - val_loss: 0.3107 - val_acc: 0.8698\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 46s - loss: 0.2504 - acc: 0.9005 - val_loss: 0.2580 - val_acc: 0.8966\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 46s - loss: 0.2242 - acc: 0.9102 - val_loss: 0.2663 - val_acc: 0.8968\n",
      "Epoch 6/7\n",
      "20000/20000 [==============================] - 45s - loss: 0.1971 - acc: 0.9211 - val_loss: 0.2738 - val_acc: 0.8894\n",
      "Epoch 7/7\n",
      "20000/20000 [==============================] - 46s - loss: 0.1720 - acc: 0.9348 - val_loss: 0.2406 - val_acc: 0.9076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd736b9e198>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best1'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s     \n",
      " Accuracy: 90.04%\n"
     ]
    }
   ],
   "source": [
    "### model1\n",
    "model.load_weights('./weights/model_best1.h5')\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\" Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "pred1 = argmax(model.predict(x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 248, 512)          461312    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 248, 512)          0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 248, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 246, 128)          196736    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 246, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 246, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 244, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 244, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 244, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 242, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 242, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 242, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               1982720   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,947,998\n",
      "Trainable params: 39,947,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "### 2nd Model. Slightly different architecture from model-1\n",
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=300, input_length=250, weights=[g_word_embedding_matrix]))\n",
    "model.add(Convolution1D(filters=512, kernel_size=3))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=128, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=64, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=32, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adadelta',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 103s - loss: 0.5481 - acc: 0.6873 - val_loss: 0.3615 - val_acc: 0.8704\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.3276 - acc: 0.8642 - val_loss: 0.3468 - val_acc: 0.8488\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.2818 - acc: 0.8863 - val_loss: 0.3187 - val_acc: 0.8594\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 102s - loss: 0.2464 - acc: 0.9028 - val_loss: 0.2694 - val_acc: 0.9038\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.2201 - acc: 0.9159 - val_loss: 0.3222 - val_acc: 0.8630\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.1911 - acc: 0.9268 - val_loss: 0.3181 - val_acc: 0.8608\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 102s - loss: 0.1621 - acc: 0.9382 - val_loss: 0.2401 - val_acc: 0.9068\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.1333 - acc: 0.9515 - val_loss: 0.2556 - val_acc: 0.9048\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.1114 - acc: 0.9605 - val_loss: 0.2645 - val_acc: 0.8984\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 101s - loss: 0.0880 - acc: 0.9699 - val_loss: 0.2962 - val_acc: 0.9050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbe6df53828>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best2'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=16,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24896/25000 [============================>.] - ETA: 0s Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "### model2\n",
    "model.load_weights('./weights/model_best2.h5')\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\" Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "pred2 = argmax(model.predict(x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 39,421,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "### 3rd Model. Same architecture as model-1. Training 2nd time. \n",
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=300, input_length=250, weights=[g_word_embedding_matrix]))\n",
    "model.add(Convolution1D(filters=128, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=64, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(filters=32, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='Adadelta',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 47s - loss: 0.5496 - acc: 0.6972 - val_loss: 0.3596 - val_acc: 0.8470\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 47s - loss: 0.3292 - acc: 0.8627 - val_loss: 0.2915 - val_acc: 0.8788\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 46s - loss: 0.2812 - acc: 0.8889 - val_loss: 0.3097 - val_acc: 0.8744\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 46s - loss: 0.2501 - acc: 0.9004 - val_loss: 0.3239 - val_acc: 0.8562\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 47s - loss: 0.2230 - acc: 0.9129 - val_loss: 0.2471 - val_acc: 0.9048\n",
      "Epoch 6/7\n",
      "20000/20000 [==============================] - 47s - loss: 0.1964 - acc: 0.9244 - val_loss: 0.2444 - val_acc: 0.9050\n",
      "Epoch 7/7\n",
      "20000/20000 [==============================] - 46s - loss: 0.1733 - acc: 0.9336 - val_loss: 0.2691 - val_acc: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac9b130278>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best3'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24768/25000 [============================>.] - ETA: 0s Accuracy: 90.06%\n"
     ]
    }
   ],
   "source": [
    "# model3\n",
    "model.load_weights('./weights/model_best3.h5')\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\" Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "pred3 = argmax(model.predict(x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acccuracy Score:  0.90476\n"
     ]
    }
   ],
   "source": [
    "### Ensemble Averaging. Taking average of 3 best performing models. 2 have the same architecture and 1 is different\n",
    "\n",
    "pred = np.zeros((len(x_test), 1))\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    if pred1[i]+pred2[i]+pred3[i] in (0,1):\n",
    "        pred[i]=0\n",
    "    elif pred1[i]+pred2[i]+pred3[i] in (2,3):\n",
    "        pred[i]=1\n",
    "    \n",
    "pred_ensemble = np_utils.to_categorical(pred, 2)\n",
    "\n",
    "### This is the final accuracy score\n",
    "\n",
    "print(\" Acccuracy Score: \", accuracy_score(y_test, pred_ensemble))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "- In Dense Layers (Fully Connected Layers) every node in the layer is connected to every node in the previous layer. The final layer in a network is often a Dense Layer, which contains a single node for each target class in the model.\n",
    "- LSTMs are a special kind of Recurrent Neural Network that are capable of learning long-term dependencies. They help preserve the error that can be backpropagated through time and layers. And this allows the RNN to learn over many time steps thereby learning causes and effects. They have a forgetting mechanism and a saving mechanism to maintain a long short term memmory. They work well with sequenced data.\n",
    "- Convolution Layers apply a convolution operation to the inputs and pass the result to the next layer. It applies this convolution operation using filters that pass over the inputs and each one passes on a different ouptut. Convolution layers are able to capture good features from raw presented input data. \n",
    "\n",
    "2.\n",
    "- ReLU is a non-linear activation function that applies to the outputs of a layer, such that wherever a negative value occurs, it is swapped with 0 and the positive values are passed as is. This helps mathematically, by keeping learned values from getting stuck near 0 or blowing up towards infinity.\n",
    "- Dropout is a technique where randomly selected neurons are ignored during training, i.e. their outputs are not propagated forward and any weight updates are not applied during the backward pass. This is only done during training, and not testing, as a regularization technique to reduce overfitting on training data.\n",
    "- Softmax is a non-linear activation function (normalized exponential function) that essentially maps a N-dimensional vector of real values to another N-dimensional vector of real values, but those are in the range [0,1] and they all sum up to 1. It is often used in the final layer of a neural network classifier.\n",
    "\n",
    "3.\n",
    "- The simple Dense layer network trains very fast. It has very few parameters, but it overfits easily and quickly. \n",
    "- Stacking the Dense layers does not create much difference in the performance. It has about the same run-time, since the parameters increase is insignificant. It also overfits quickly.\n",
    "- The LSTM network has much less parameters compared to a pure Dense layer network. The accuracy stays around the same, but the model continues to overfit. However, the training time is many times over the previous networks. This could be used as a trade-off parameter, while choosing models.\n",
    "- Using pre-trained word embeddings has helped in pushing up the accuracy by 2-3 points, which was expected by the use of those embeddings. And the training time has increased significantly over the previous LSTM network. The overfit reduction is very little after applying dropout. Also, the parameters have increased exponentially due to the use of word embeddings, which would mean higher memory footprint. \n",
    "- The 3 layer stacked CNN seems to have worked the best. It has slight increase in parameters over the previous model, but the training time is much less. Also, there does not seem to have been much reduction in the overfitting, with the use of dropout. \n",
    "- In part-F when experimenting different architectures, I tried the optimizer='Adadelta'. The training time increased by a few seconds, but the reduction in overfit was very significant, and there was slight improvement in the accuracy. \n",
    " \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
